{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/francescadima1999/Esempio1/blob/main/Tuplet_Extractorssss_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhEDYalWkv7b"
      },
      "source": [
        "## Step 1: Installazione dei packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VuenKPqoNfS",
        "outputId": "9ae65e23-1659-4c71-dedb-ed63ca99f7f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.11.12\n"
          ]
        }
      ],
      "source": [
        "!python -V  #Python 3.10.12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nj8FZCXshH-Y",
        "outputId": "48af5548-d793-45a7-e1ac-085a71a38f4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version # find the CUDA driver build above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-oKv5ARNT9a"
      },
      "source": [
        "`cu121`: CUDA 12.1 - `cu122`: CUDA 12.2 - `cu123`: CUDA 12.3 - `cu124`: CUDA 12.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X15zY5debUpP",
        "outputId": "d79540a4-50d5-45f5-ed29-85c5aab2f3b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://jllllll.github.io/llama-cpp-python-cuBLAS-wheels/AVX2/cu122\n",
            "Collecting llama-cpp-python\n",
            "  Downloading https://github.com/jllllll/llama-cpp-python-cuBLAS-wheels/releases/download/wheels/llama_cpp_python-0.2.26%2Bcu122-cp311-cp311-manylinux_2_31_x86_64.whl (28.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.1/28.1 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=4.5.0 (from llama-cpp-python)\n",
            "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting numpy>=1.20.0 (from llama-cpp-python)\n",
            "  Downloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: typing-extensions, numpy, diskcache, llama-cpp-python\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.13.2\n",
            "    Uninstalling typing_extensions-4.13.2:\n",
            "      Successfully uninstalled typing_extensions-4.13.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.5 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.5 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed diskcache-5.6.3 llama-cpp-python-0.2.26+cu122 numpy-2.2.5 typing-extensions-4.13.2\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Collecting torch==2.3.0\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.3.0%2Bcu121-cp311-cp311-linux_x86_64.whl (781.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.0/781.0 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Collecting torchaudio==2.3.0\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.3.0%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m102.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m122.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.3.0 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/triton-2.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0) (12.5.82)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.2.5)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp311-cp311-linux_x86_64.whl (7.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m110.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.20.0%2Bcu121-cp311-cp311-linux_x86_64.whl (7.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.19.1%2Bcu121-cp311-cp311-linux_x86_64.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.19.0%2Bcu121-cp311-cp311-linux_x86_64.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.18.1%2Bcu121-cp311-cp311-linux_x86_64.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.18.0%2Bcu121-cp311-cp311-linux_x86_64.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m99.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.3.0) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.3.0) (1.3.0)\n",
            "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cu124\n",
            "    Uninstalling torchvision-0.21.0+cu124:\n",
            "      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.6.0+cu124\n",
            "    Uninstalling torchaudio-2.6.0+cu124:\n",
            "      Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.3.0+cu121 torchaudio-2.3.0+cu121 torchvision-0.18.0+cu121 triton-2.3.0\n"
          ]
        }
      ],
      "source": [
        "# Install key libraries for LLM\n",
        "\n",
        "#Install llama-cpp-python with CUDA per utilizzare la GPU\n",
        "!set CMAKE_ARGS=-DGGML_CUDA=on\n",
        "!set FORCE_CMAKE=1\n",
        "\n",
        "#Install llama-cpp-python, cuda-enabled package\n",
        "!python -m pip install llama-cpp-python --prefer-binary --extra-index-url=https://jllllll.github.io/llama-cpp-python-cuBLAS-wheels/AVX2/cu122 --force-reinstall\n",
        "\n",
        "#Install pytorch-related, cuda-enabled package\n",
        "!pip install torch==2.3.0 torchvision torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu121"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efJ_g2etSvLD"
      },
      "source": [
        "## Step 2: Download del LLM da hugging face\n",
        "\n",
        "Use the \"hf_hub_download\" function to download models on huggingface.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "kpXQGhHlij6q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Modello LLAMA"
      ],
      "metadata": {
        "id": "fE0-947olU53"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171,
          "referenced_widgets": [
            "7c7c0dc7893548d4bc60eb2cf2f903be",
            "f2392a13b8b34aa6a3d3651239f72fc4",
            "5a89180e243444a7aa07b7ce120fca7d",
            "d9d17a354dec40a4a5ebab45e03f146a",
            "7d41e71f38f34c329066da674280bff8",
            "8731ed9695014cd18158ad8ecec47103",
            "2c18fdd6906f4ff082cee7b2c2fb7834",
            "bebf86f212ae47deba055f4ccc356d3f",
            "4ebd766e91f3457086dc03610b2c1d91",
            "c2e132f407ff4ccc820b8068b1671dbb",
            "ebc08eb8519a4eb0997d2c216ec756d7"
          ]
        },
        "id": "WsYot3Rz1NVf",
        "outputId": "d757986f-b78d-400d-eb17-c18d97d1f1c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Llama-3-8B-Instruct-v0.10.Q8_0.gguf:   0%|          | 0.00/8.54G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c7c0dc7893548d4bc60eb2cf2f903be"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My model path: models/Llama-3-8B-Instruct-v0.10.Q8_0.gguf\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# Define the model name and file\n",
        "model_name = \"MaziyarPanahi/Llama-3-8B-Instruct-v0.10-GGUF\"\n",
        "\n",
        "#Quantizzazione 1\n",
        "#model_file = \"Llama-3-8B-Instruct-v0.10.Q4_K_M.gguf\" # this is the specific model file we'll use in this example. It's a 4-bit quant, but other levels of quantization are available in the model repo if preferred\n",
        "\n",
        "#Quantizzazione 2\n",
        "# model_file = \"Llama-3-8B-Instruct-v0.10.Q5_K_M.gguf\"\n",
        "\n",
        "#Quantizzazione 3\n",
        "model_file = \"Llama-3-8B-Instruct-v0.10.Q8_0.gguf\"\n",
        "\n",
        "\n",
        "# Download the model from Hugging Face Hub\n",
        "model_path = hf_hub_download(\n",
        "    model_name,\n",
        "    filename=model_file,\n",
        "    local_dir='models/',  # Download the model to the \"models\" folder\n",
        "    )\n",
        "#Copia il path nel prossimo comando\n",
        "print(\"My model path:\", model_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del llm #Ripulisci la gpu dai modelli caricati"
      ],
      "metadata": {
        "id": "E0Xn9dRQMEvW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "70f0450f-c59f-4cd3-9b37-e66a60da9c35"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'llm' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-fe8724b66ae1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mllm\u001b[0m \u001b[0;31m#Ripulisci la gpu dai modelli caricati\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'llm' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFMXgvWzckbU"
      },
      "source": [
        "Note that BLAS = 1 means GPU is enabled:\n",
        "*   AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4GldNtm_s-P"
      },
      "source": [
        "## Step 3: Utilizzo del LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBMlNefW2qnq"
      },
      "source": [
        "####Import del dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0_onscHeKt4",
        "outputId": "6dc311c0-5230-4837-cf51-682e13b0436c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     ID                                                URL TITLE  \\\n",
            "0  6430  https://twitter.com/leroylovesusa/status/80150...   NaN   \n",
            "1  5180  https://twitter.com/leroylovesusa/status/81772...   NaN   \n",
            "2  9234  https://twitter.com/worldofhashtags/status/765...   NaN   \n",
            "\n",
            "            SOURCE                                               TEXT  \\\n",
            "0    leroylovesusa  RT @RealMrsPJNET: MT @prolife_wife: Call me cr...   \n",
            "1    leroylovesusa  RT @Amyloukingery: Ingrid Carlqvist Moment: Ho...   \n",
            "2  worldofhashtags  RT @PresidentJeffPJ: Fact Checkers Embarrassed...   \n",
            "\n",
            "                                            CAMPAIGN THREAT ACTOR     TYPE  \n",
            "0  Russian troll accounts during 2016 U.S. presid...          NaN  TWITTER  \n",
            "1  Russian troll accounts during 2016 U.S. presid...          NaN  TWITTER  \n",
            "2  Russian troll accounts during 2016 U.S. presid...          NaN  TWITTER  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel('train.xlsx')\n",
        "print(df.head(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBDLfK8g2tHr"
      },
      "source": [
        "**Separazione delle colonne del dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "VncP0-XokqtB"
      },
      "outputs": [],
      "source": [
        "# URL , TITOLO, SOURCE, TESTO, CAMPAGNA , THREAT ACTOR, TIPO\n",
        "colonna_id= df['ID']\n",
        "colonna_url = df['URL']\n",
        "colonna_titolo = df['TITLE']\n",
        "colonna_source = df['SOURCE']\n",
        "colonna_testo = df['TEXT']\n",
        "colonna_campagna= df['CAMPAIGN']\n",
        "colonna_threat_actor = df['THREAT ACTOR']\n",
        "colonna_tipo = df['TYPE']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIJAgUxHvyfj",
        "outputId": "f837889e-508f-4997-8d72-ddce63d6c6d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RT @RealMrsPJNET: MT @prolife_wife: Call me crazy. \n",
            " #ProLife https://t.co/jPimeEYKJX #UnbornLivesMatter #PJNET\n",
            "111\n"
          ]
        }
      ],
      "source": [
        "print(colonna_testo[0])\n",
        "print(len(colonna_testo[0]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "llm = Llama(model_path=model_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktZjgs896glD",
        "outputId": "115cc0b7-7e35-411e-a7db-7c095b6fd1b0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqeh1Aej6BO6"
      },
      "source": [
        "####Setup temperature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "dK8lgU85hM-A"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache() # Clear GPU cache"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run Temperatura=0**"
      ],
      "metadata": {
        "id": "QwVgw5ueul6l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "xSyxPuAXgfut"
      },
      "outputs": [],
      "source": [
        "def model_run(user_input):\n",
        "    # Definisci la sequenza di stop\n",
        "    stop_sequence = [\"##END LIST##\",\"#END LIST#\",\"END LIST\"]  # Sostituisci con la sequenza desiderata\n",
        "\n",
        "    # Crea la richiesta di completamento\n",
        "    output = llm.create_chat_completion(\n",
        "        messages= [\n",
        "            {\"role\": \"system\", \"content\": \"You are a bot that extracts representative elements of its high-level objective from the text\"},\n",
        "            {\"role\": \"user\", \"content\": user_input}\n",
        "        ],\n",
        "        temperature=0,\n",
        "        top_p=0.95,\n",
        "        stop=stop_sequence\n",
        "    )\n",
        "\n",
        "    return output['choices'][0]['message']['content']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run Temperatura = 0.3**"
      ],
      "metadata": {
        "id": "x2nKlWPv0iI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_run(user_input):\n",
        "    # Definisci la sequenza di stop\n",
        "    stop_sequence = [\"##END LIST##\",\"#END LIST#\",\"END LIST\"]  # Sostituisci con la sequenza desiderata\n",
        "\n",
        "    # Crea la richiesta di completamento\n",
        "    output = llm.create_chat_completion(\n",
        "        messages= [\n",
        "            {\"role\": \"system\", \"content\": \"You are a bot that extracts representative elements of its high-level objective from the text\"},\n",
        "            {\"role\": \"user\", \"content\": user_input}\n",
        "        ],\n",
        "        temperature=0.3,\n",
        "        top_p=0.95,\n",
        "        stop=stop_sequence\n",
        "    )\n",
        "\n",
        "    return output['choices'][0]['message']['content']"
      ],
      "metadata": {
        "id": "6LE-psjH0k95"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run Temperatura = 0.6**"
      ],
      "metadata": {
        "id": "agYdcjGgAevP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_run(user_input):\n",
        "    # Definisci la sequenza di stop\n",
        "    stop_sequence = [\"##END LIST##\",\"#END LIST#\",\"END LIST\"]  # Sostituisci con la sequenza desiderata\n",
        "\n",
        "    # Crea la richiesta di completamento\n",
        "    output = llm.create_chat_completion(\n",
        "        messages= [\n",
        "            {\"role\": \"system\", \"content\": \"You are a bot that extracts representative elements of its high-level objective from the text\"},\n",
        "            {\"role\": \"user\", \"content\": user_input}\n",
        "        ],\n",
        "        temperature=0.6,\n",
        "        top_p=0.95,\n",
        "        stop=stop_sequence\n",
        "    )\n",
        "\n",
        "    return output['choices'][0]['message']['content']"
      ],
      "metadata": {
        "id": "5QP2s4AyAdbw"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Funzione di estrazione dal testo"
      ],
      "metadata": {
        "id": "NUeUls9XBS1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def extract_tuples(text):\n",
        "    # Trova la parte del testo prima di \"##END LIST## (1)\"\n",
        "    before_note = text.split(\"##END LIST## (1)\")[0]\n",
        "\n",
        "    # Regex per trovare le tuple nel formato \"soggetto - verbo - oggetto\"\n",
        "    # Ignora il numero iniziale seguito da un punto o altri caratteri\n",
        "    pattern = r'\\d*\\.*\\s*(.*? - .*? - .*?)\\n'\n",
        "\n",
        "    # Trova tutte le tuple\n",
        "    matches = re.findall(pattern, before_note)\n",
        "\n",
        "    return matches"
      ],
      "metadata": {
        "id": "3EfB8cSiuzuN"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHfbvCSenpwi"
      },
      "source": [
        "####Debug singolo run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "IY3ks3DJejoa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa15a64d-36ec-4167-c950-ba2a40cc1f57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " <</SYS>>\n",
            "\n",
            "\n",
            "<</SYS>> Process the provided text as instructed above and identify the main actions and relationships between the entities mentioned in the text by extracting tuples in the subject-verb-object form. \n",
            "Here are the extracted tuples:\n",
            "\n",
            "1. I - am not - a licensed healthcare professional\n",
            "2. I - cannot - diagnose or treat medical conditions\n",
            "3. Three commonly suggested strategies - are - for coping with depression\n",
            "4. One of the most effective strategies - is - to seek professional help\n",
            "5. Seek - professional help - from a licensed therapist or psychiatrist\n",
            "6. They - can - help you identify the root causes and work with you\n",
            "7\n"
          ]
        }
      ],
      "source": [
        "user_input = f'''Read the following text and identify all the tuples in the subject-verb-object form. The tuples should reflect the main actions and relationships between the entities mentioned in the text. Follow these steps:\n",
        "\n",
        "              Identify the subject of the action.\n",
        "              Identify the verb that describes the action or relationship.\n",
        "              Identify the object or destination of the action.\n",
        "              Return the tuples in this format: Subject - Verb - Object.\n",
        "\n",
        "\n",
        "              Example:\n",
        "              Text: 'John gave a book to Mary.'\n",
        "              Tuple: 'John - gave - a book to Mary'\n",
        "              ##END LIST##\n",
        "\n",
        "              Apply this method to the following text:, {colonna_testo[9]} '''\n",
        "\n",
        "\n",
        "# Invia il messaggio e stampa la risposta\n",
        "response = model_run(user_input)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuples = extract_tuples(response)\n",
        "print(tuples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9OCMcus9PTh",
        "outputId": "4f2007ad-a895-4d2d-ede8-a93ce74df61d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1. I - am not - a licensed healthcare professional', 'I - cannot - diagnose or treat medical conditions', 'Three commonly suggested strategies - are - for coping with depression', 'One of the most effective strategies - is - to seek professional help', 'Seek - professional help - from a licensed therapist or psychiatrist', 'They - can - help you identify the root causes and work with you']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Run Iterato"
      ],
      "metadata": {
        "id": "GskMXLc5BXtf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKtGNF3nRG4B",
        "outputId": "4e3695c3-b5ef-4a9e-e584-0ac6b53d4d0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " >>\n",
            "\n",
            "The extracted tuples will be:\n",
            "Call - me - crazy\n",
            "Call - me - crazy (Note that 'me' is the subject of the action, and 'crazy' is an object describing or characterizing the call, not a direct object)\n",
            "No more tuples to extract as there's no more actions described in this text.\n",
            "\n",
            "-----------------------FINE--------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " <<SYS>>\n",
            "\n",
            "\n",
            "Subject - Verb - Object.\n",
            "1. Amyloukingery - posted - a tweet\n",
            "2. Amyloukingery - shared - 'Ingrid Carlqvist Moment'\n",
            "3. Ingrid Carlqvist - is the author of - 'How Sweden Became Absurdistan'\n",
            "4. Ingrid Carlqvist - has an article - 'How Sweden Became Absurdistan' \n",
            "5. Sweden - became - Absurdistan\n",
            "6. Amyloukingery - shared - a link https://t.co/i3tVdiKM6K\n",
            "#\n",
            "-----------------------FINE--------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " >>\n",
            "\n",
            "The provided text does not contain any action or relationship that can be parsed into a tuple in the subject-verb-object format. The text appears to be a tweet quote and some hashtags, without any descriptive sentences or clauses.\n",
            "\n",
            "<</SYS>> \n",
            "-----------------------FINE--------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "\n",
            "Extract representative elements of its high-level objective from the text in the subject-verb-object form.\n",
            "\n",
            "[INST] <<SYS>> \n",
            "Please provide your output in the format 'Subject - Verb - Object'. The output should be a list of tuples reflecting main actions and relationships in the given text. After the listing, write \"\n",
            "-----------------------FINE--------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " <<SYS>> \n",
            "Extracting representative elements of its high-level objective from the text\n",
            "<</SYS>>\n",
            "\n",
            "Identifying subject-verb-object tuples:\n",
            "\n",
            "1. 'RT - posted - '\n",
            "   (Incomplete tuple as the object is missing, likely a URL or a tweet)\n",
            "2. 'HawkinsUSA - tweeted - Well, it worked! and https://t.co/LrDyEB50Xt'\n",
            "   #\n",
            "-----------------------FINE--------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " <<SYS>>\n",
            "\n",
            "No tuples in subject-verb-object form can be extracted from this text as it does not contain a coherent statement of an action or relationship. The text appears to be a tweet with a mix of hashtags and mentions, but lacks a structured sentence that would yield meaningful SVO tuples.\n",
            "\n",
            "<</SYS>> \n",
            "-----------------------FINE--------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " <<SYS>>\n",
            "\n",
            "\n",
            "The task is to extract representative elements of its high-level objective from a tweet.\n",
            "\n",
            "After analyzing the tweet, I will provide the tuples in the subject-verb-object form that reflect the main actions and relationships between entities mentioned in the tweet. Here are the extracted tuples:\n",
            "\n",
            "Ryanz - tweeted - #WeedTV Stonefeld https://t.co/e6gW9cAgFV\n",
            "#\n",
            "-----------------------FINE--------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " >>\n",
            "<</SYS>>\n",
            "\n",
            "Extracted tuples:\n",
            "\n",
            "Children - Think - That being an adult is awesome\n",
            "Anthony - say/think (implied) - Being an adult is highly overrated #\n",
            "-----------------------FINE--------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " [</s>>\n",
            "\n",
            "\n",
            "Here, we don't have any verbs or actions mentioned in the tweet. It seems to be a simple sharing of an opinion with hashtags and a link not providing any substantial information for extracting subject-verb-object tuples.\n",
            "\n",
            "\n",
            "-----------------------FINE--------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " <<SYS>>\n",
            "\n",
            "Here are the extracted tuples:\n",
            "\n",
            "As an AI - is - a language model\n",
            "I - am - not - a licensed - healthcare professional - and - cannot - diagnose - or - treat - medical conditions\n",
            "I - suggest - strategies - for - coping - with - depression\n",
            "One - of - the - most - effective - strategies - is - to - seek - professional - help\n",
            "Seeking - Professional - Help - is - to - seek - professional - help\n",
            "It - can - help - you - identify - the - root - causes - of - your - depression\n",
            "They -\n",
            "-----------------------FINE--------------------------------------\n",
            "response run time is:  851.9316987991333\n",
            "                                                TUPLA  ID ARTICOLO  \\\n",
            "0                                   Call - me - crazy         6430   \n",
            "1   Call - me - crazy (Note that 'me' is the subje...         6430   \n",
            "2                            Subject - Verb - Object.         5180   \n",
            "3                    Amyloukingery - posted - a tweet         5180   \n",
            "4   Amyloukingery - shared - 'Ingrid Carlqvist Mom...         5180   \n",
            "5   Ingrid Carlqvist - is the author of - 'How Swe...         5180   \n",
            "6   Ingrid Carlqvist - has an article - 'How Swede...         5180   \n",
            "7                       Sweden - became - Absurdistan         5180   \n",
            "8   Amyloukingery - shared - a link https://t.co/i...         5180   \n",
            "9                                 1. 'RT - posted - '         5020   \n",
            "10  2. 'HawkinsUSA - tweeted - Well, it worked! an...         5020   \n",
            "11  Ryanz - tweeted - #WeedTV Stonefeld https://t....         7046   \n",
            "12  Children - Think - That being an adult is awesome         9860   \n",
            "13                   As an AI - is - a language model        12070   \n",
            "14  I - am - not - a licensed - healthcare profess...        12070   \n",
            "15  I - suggest - strategies - for - coping - with...        12070   \n",
            "16  One - of - the - most - effective - strategies...        12070   \n",
            "17  Seeking - Professional - Help - is - to - seek...        12070   \n",
            "18  It - can - help - you - identify - the - root ...        12070   \n",
            "\n",
            "                                             CAMPAGNA  \n",
            "0   Russian troll accounts during 2016 U.S. presid...  \n",
            "1   Russian troll accounts during 2016 U.S. presid...  \n",
            "2   Russian troll accounts during 2016 U.S. presid...  \n",
            "3   Russian troll accounts during 2016 U.S. presid...  \n",
            "4   Russian troll accounts during 2016 U.S. presid...  \n",
            "5   Russian troll accounts during 2016 U.S. presid...  \n",
            "6   Russian troll accounts during 2016 U.S. presid...  \n",
            "7   Russian troll accounts during 2016 U.S. presid...  \n",
            "8   Russian troll accounts during 2016 U.S. presid...  \n",
            "9   Russian troll accounts during 2016 U.S. presid...  \n",
            "10  Russian troll accounts during 2016 U.S. presid...  \n",
            "11  Russian troll accounts during 2016 U.S. presid...  \n",
            "12  Russian troll accounts during 2016 U.S. presid...  \n",
            "13                                  Medical Fake News  \n",
            "14                                  Medical Fake News  \n",
            "15                                  Medical Fake News  \n",
            "16                                  Medical Fake News  \n",
            "17                                  Medical Fake News  \n",
            "18                                  Medical Fake News  \n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "#print(content)\n",
        "\n",
        "# Calculate runtime\n",
        "#runtime = end_time - start_time\n",
        "#print(\"response run time is: \", runtime)\n",
        "\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "for i in range(10):\n",
        "\n",
        "  torch.cuda.empty_cache() # Clear GPU cache\n",
        "  user_input = f'''Read the following text and identify all the tuples in the subject-verb-object form. The tuples should reflect the main actions and relationships between the entities mentioned in the text. Follow these steps:\n",
        "\n",
        "              Identify the subject of the action.\n",
        "              Identify the verb that describes the action or relationship.\n",
        "              Identify the object or destination of the action.\n",
        "              Return the tuples in this format: Subject - Verb - Object.\n",
        "              Print \"END LIST\" at the end of the list.\n",
        "\n",
        "              Example:\n",
        "              Text: 'John gave a book to Mary.'\n",
        "              Tuple: 'John - gave - a book to Mary'\n",
        "              ##END LIST##\n",
        "\n",
        "              Apply this method to the following text:, {colonna_testo[i]} '''\n",
        "\n",
        "\n",
        "  # Invia il messaggio e stampa la risposta\n",
        "  response = model_run(user_input)\n",
        "  print(response)\n",
        "  print(\"-----------------------FINE--------------------------------------\")\n",
        "  # Estrazione delle tuple complete\n",
        "  tuples = extract_tuples(response)\n",
        "\n",
        "  if i == 0:\n",
        "    # Creare un DataFrame\n",
        "    df_tuple = pd.DataFrame(tuples, columns=['TUPLA'])\n",
        "     # Aggiungere la colonna 'ID' con il valore di colonna_ID[0]\n",
        "    df_tuple['ID ARTICOLO'] = colonna_id[i]\n",
        "    df_tuple['CAMPAGNA'] = colonna_campagna[i]\n",
        "  else:\n",
        "    # Create temporary DataFrames for new data\n",
        "    df_temp = pd.DataFrame({'TUPLA': tuples, 'ID ARTICOLO': colonna_id[i], 'CAMPAGNA': colonna_campagna[i]})\n",
        "    # Concatenate the temporary DataFrame with the main DataFrame\n",
        "    df_tuple = pd.concat([df_tuple, df_temp], ignore_index=True)\n",
        "\n",
        "# Stop the timer\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate runtime\n",
        "runtime = end_time - start_time\n",
        "print(\"response run time is: \", runtime)\n",
        "print(df_tuple.head(50))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wS4JrU-RrtH",
        "outputId": "9b362efb-9da2-4b6c-8fd1-312e61d515da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "851.9316987991333\n",
            "File Excel creato con successo!\n"
          ]
        }
      ],
      "source": [
        "# Salvare il DataFrame in un file Excel\n",
        "df_tuple.to_excel('TuplesExtracted_Q8_06TEMP.xlsx', index=False)\n",
        "print(runtime)\n",
        "print(\"File Excel creato con successo!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RT_Q5KM_0TEMP=371.50677728652954\n",
        "RT_Q5KM_03TEMP=385.1130666732788\n",
        "RT_Q5KM_06TEMP=394.9632875919342\n",
        "RT_Q8_0TEMP=340.24108028411865\n",
        "RT_Q8_03TEMP= 390.331839799881\n",
        "RT_Q8_06TEMP= 388.83690667152405"
      ],
      "metadata": {
        "id": "TZYg7oi2vPuU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7c7c0dc7893548d4bc60eb2cf2f903be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f2392a13b8b34aa6a3d3651239f72fc4",
              "IPY_MODEL_5a89180e243444a7aa07b7ce120fca7d",
              "IPY_MODEL_d9d17a354dec40a4a5ebab45e03f146a"
            ],
            "layout": "IPY_MODEL_7d41e71f38f34c329066da674280bff8"
          }
        },
        "f2392a13b8b34aa6a3d3651239f72fc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8731ed9695014cd18158ad8ecec47103",
            "placeholder": "​",
            "style": "IPY_MODEL_2c18fdd6906f4ff082cee7b2c2fb7834",
            "value": "Llama-3-8B-Instruct-v0.10.Q8_0.gguf: 100%"
          }
        },
        "5a89180e243444a7aa07b7ce120fca7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bebf86f212ae47deba055f4ccc356d3f",
            "max": 8540770944,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4ebd766e91f3457086dc03610b2c1d91",
            "value": 8540770944
          }
        },
        "d9d17a354dec40a4a5ebab45e03f146a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2e132f407ff4ccc820b8068b1671dbb",
            "placeholder": "​",
            "style": "IPY_MODEL_ebc08eb8519a4eb0997d2c216ec756d7",
            "value": " 8.54G/8.54G [00:48&lt;00:00, 252MB/s]"
          }
        },
        "7d41e71f38f34c329066da674280bff8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8731ed9695014cd18158ad8ecec47103": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c18fdd6906f4ff082cee7b2c2fb7834": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bebf86f212ae47deba055f4ccc356d3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ebd766e91f3457086dc03610b2c1d91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c2e132f407ff4ccc820b8068b1671dbb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebc08eb8519a4eb0997d2c216ec756d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}