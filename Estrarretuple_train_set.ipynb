{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/francescadima1999/Esempio1/blob/main/Estrarretuple_train_set.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhEDYalWkv7b"
      },
      "source": [
        "## Step 1: Installazione dei packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VuenKPqoNfS",
        "outputId": "9ae65e23-1659-4c71-dedb-ed63ca99f7f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.11.12\n"
          ]
        }
      ],
      "source": [
        "!python -V  #Python 3.10.12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nj8FZCXshH-Y",
        "outputId": "48af5548-d793-45a7-e1ac-085a71a38f4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version # find the CUDA driver build above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-oKv5ARNT9a"
      },
      "source": [
        "`cu121`: CUDA 12.1 - `cu122`: CUDA 12.2 - `cu123`: CUDA 12.3 - `cu124`: CUDA 12.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X15zY5debUpP",
        "outputId": "d79540a4-50d5-45f5-ed29-85c5aab2f3b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://jllllll.github.io/llama-cpp-python-cuBLAS-wheels/AVX2/cu122\n",
            "Collecting llama-cpp-python\n",
            "  Downloading https://github.com/jllllll/llama-cpp-python-cuBLAS-wheels/releases/download/wheels/llama_cpp_python-0.2.26%2Bcu122-cp311-cp311-manylinux_2_31_x86_64.whl (28.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.1/28.1 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=4.5.0 (from llama-cpp-python)\n",
            "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting numpy>=1.20.0 (from llama-cpp-python)\n",
            "  Downloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: typing-extensions, numpy, diskcache, llama-cpp-python\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.13.2\n",
            "    Uninstalling typing_extensions-4.13.2:\n",
            "      Successfully uninstalled typing_extensions-4.13.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.5 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.5 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed diskcache-5.6.3 llama-cpp-python-0.2.26+cu122 numpy-2.2.5 typing-extensions-4.13.2\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Collecting torch==2.3.0\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.3.0%2Bcu121-cp311-cp311-linux_x86_64.whl (781.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.0/781.0 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Collecting torchaudio==2.3.0\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.3.0%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m102.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m122.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.3.0 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/triton-2.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
            "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m151.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install key libraries for LLM\n",
        "\n",
        "#Install llama-cpp-python with CUDA per utilizzare la GPU\n",
        "!set CMAKE_ARGS=-DGGML_CUDA=on\n",
        "!set FORCE_CMAKE=1\n",
        "\n",
        "#Install llama-cpp-python, cuda-enabled package\n",
        "!python -m pip install llama-cpp-python --prefer-binary --extra-index-url=https://jllllll.github.io/llama-cpp-python-cuBLAS-wheels/AVX2/cu122 --force-reinstall\n",
        "\n",
        "#Install pytorch-related, cuda-enabled package\n",
        "!pip install torch==2.3.0 torchvision torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu121"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efJ_g2etSvLD"
      },
      "source": [
        "## Step 2: Download del LLM da hugging face\n",
        "\n",
        "Use the \"hf_hub_download\" function to download models on huggingface.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "kpXQGhHlij6q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Modello LLAMA"
      ],
      "metadata": {
        "id": "fE0-947olU53"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsYot3Rz1NVf",
        "outputId": "a102109c-75b8-4b01-c7ae-bbfb393cf9ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My model path: models/Llama-3-8B-Instruct-v0.10.Q8_0.gguf\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# Define the model name and file\n",
        "model_name = \"MaziyarPanahi/Llama-3-8B-Instruct-v0.10-GGUF\"\n",
        "\n",
        "#Quantizzazione 1\n",
        "#model_file = \"Llama-3-8B-Instruct-v0.10.Q4_K_M.gguf\" # this is the specific model file we'll use in this example. It's a 4-bit quant, but other levels of quantization are available in the model repo if preferred\n",
        "\n",
        "#Quantizzazione 2\n",
        "# model_file = \"Llama-3-8B-Instruct-v0.10.Q5_K_M.gguf\"\n",
        "\n",
        "#Quantizzazione 3\n",
        "#model_file = \"Llama-3-8B-Instruct-v0.10.Q8_0.gguf\"\n",
        "\n",
        "\n",
        "# Download the model from Hugging Face Hub\n",
        "model_path = hf_hub_download(\n",
        "    model_name,\n",
        "    filename=model_file,\n",
        "    local_dir='models/',  # Download the model to the \"models\" folder\n",
        "    )\n",
        "#Copia il path nel prossimo comando\n",
        "print(\"My model path:\", model_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del llm #Ripulisci la gpu dai modelli caricati"
      ],
      "metadata": {
        "id": "E0Xn9dRQMEvW"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFMXgvWzckbU"
      },
      "source": [
        "Note that BLAS = 1 means GPU is enabled:\n",
        "*   AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4GldNtm_s-P"
      },
      "source": [
        "## Step 3: Utilizzo del LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBMlNefW2qnq"
      },
      "source": [
        "####Import del dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0_onscHeKt4",
        "outputId": "43e5188f-6ff2-4de5-dcb6-1c152ec4a0aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     ID                                                URL TITLE  \\\n",
            "0  6430  https://twitter.com/leroylovesusa/status/80150...   NaN   \n",
            "1  5180  https://twitter.com/leroylovesusa/status/81772...   NaN   \n",
            "2  9234  https://twitter.com/worldofhashtags/status/765...   NaN   \n",
            "\n",
            "            SOURCE                                               TEXT  \\\n",
            "0    leroylovesusa  RT @RealMrsPJNET: MT @prolife_wife: Call me cr...   \n",
            "1    leroylovesusa  RT @Amyloukingery: Ingrid Carlqvist Moment: Ho...   \n",
            "2  worldofhashtags  RT @PresidentJeffPJ: Fact Checkers Embarrassed...   \n",
            "\n",
            "                                            CAMPAIGN THREAT ACTOR     TYPE  \n",
            "0  Russian troll accounts during 2016 U.S. presid...          NaN  TWITTER  \n",
            "1  Russian troll accounts during 2016 U.S. presid...          NaN  TWITTER  \n",
            "2  Russian troll accounts during 2016 U.S. presid...          NaN  TWITTER  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel('train.xlsx')\n",
        "print(df.head(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBDLfK8g2tHr"
      },
      "source": [
        "**Separazione delle colonne del dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VncP0-XokqtB"
      },
      "outputs": [],
      "source": [
        "# URL , TITOLO, SOURCE, TESTO, CAMPAGNA , THREAT ACTOR, TIPO\n",
        "colonna_id= df['ID']\n",
        "colonna_url = df['URL']\n",
        "colonna_titolo = df['TITLE']\n",
        "colonna_source = df['SOURCE']\n",
        "colonna_testo = df['TEXT']\n",
        "colonna_campagna= df['CAMPAIGN']\n",
        "colonna_threat_actor = df['THREAT ACTOR']\n",
        "colonna_tipo = df['TYPE']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIJAgUxHvyfj",
        "outputId": "79ff611e-ad99-4c3b-a2d3-f37d3923a8aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RT @RealMrsPJNET: MT @prolife_wife: Call me crazy. \n",
            " #ProLife https://t.co/jPimeEYKJX #UnbornLivesMatter #PJNET\n",
            "111\n"
          ]
        }
      ],
      "source": [
        "print(colonna_testo[0])\n",
        "print(len(colonna_testo[0]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "llm = Llama(model_path=model_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "ktZjgs896glD",
        "outputId": "3d128d32-d52b-42c1-fdb6-9c0fdf0d7b79"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'llama_cpp'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-ba6b71905853>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_cpp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLlama\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLlama\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llama_cpp'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqeh1Aej6BO6"
      },
      "source": [
        "####Setup temperature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "dK8lgU85hM-A"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache() # Clear GPU cache"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run Temperatura=0**"
      ],
      "metadata": {
        "id": "QwVgw5ueul6l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "xSyxPuAXgfut"
      },
      "outputs": [],
      "source": [
        "def model_run(user_input):\n",
        "    # Definisci la sequenza di stop\n",
        "    stop_sequence = [\"##END LIST##\",\"#END LIST#\",\"END LIST\"]  # Sostituisci con la sequenza desiderata\n",
        "\n",
        "    # Crea la richiesta di completamento\n",
        "    output = llm.create_chat_completion(\n",
        "        messages= [\n",
        "            {\"role\": \"system\", \"content\": \"You are a bot that extracts representative elements of its high-level objective from the text\"},\n",
        "            {\"role\": \"user\", \"content\": user_input}\n",
        "        ],\n",
        "        temperature=0,\n",
        "        top_p=0.95,\n",
        "        stop=stop_sequence\n",
        "    )\n",
        "\n",
        "    return output['choices'][0]['message']['content']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run Temperatura = 0.3**"
      ],
      "metadata": {
        "id": "x2nKlWPv0iI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_run(user_input):\n",
        "    # Definisci la sequenza di stop\n",
        "    stop_sequence = [\"##END LIST##\",\"#END LIST#\",\"END LIST\"]  # Sostituisci con la sequenza desiderata\n",
        "\n",
        "    # Crea la richiesta di completamento\n",
        "    output = llm.create_chat_completion(\n",
        "        messages= [\n",
        "            {\"role\": \"system\", \"content\": \"You are a bot that extracts representative elements of its high-level objective from the text\"},\n",
        "            {\"role\": \"user\", \"content\": user_input}\n",
        "        ],\n",
        "        temperature=0.3,\n",
        "        top_p=0.95,\n",
        "        stop=stop_sequence\n",
        "    )\n",
        "\n",
        "    return output['choices'][0]['message']['content']"
      ],
      "metadata": {
        "id": "6LE-psjH0k95"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run Temperatura = 0.6**"
      ],
      "metadata": {
        "id": "agYdcjGgAevP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_run(user_input):\n",
        "    # Definisci la sequenza di stop\n",
        "    stop_sequence = [\"##END LIST##\",\"#END LIST#\",\"END LIST\"]  # Sostituisci con la sequenza desiderata\n",
        "\n",
        "    # Crea la richiesta di completamento\n",
        "    output = llm.create_chat_completion(\n",
        "        messages= [\n",
        "            {\"role\": \"system\", \"content\": \"You are a bot that extracts representative elements of its high-level objective from the text\"},\n",
        "            {\"role\": \"user\", \"content\": user_input}\n",
        "        ],\n",
        "        temperature=0.6,\n",
        "        top_p=0.95,\n",
        "        stop=stop_sequence\n",
        "    )\n",
        "\n",
        "    return output['choices'][0]['message']['content']"
      ],
      "metadata": {
        "id": "5QP2s4AyAdbw"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Funzione di estrazione dal testo"
      ],
      "metadata": {
        "id": "NUeUls9XBS1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def extract_tuples(text):\n",
        "    # Trova la parte del testo prima di \"##END LIST## (1)\"\n",
        "    before_note = text.split(\"##END LIST## (1)\")[0]\n",
        "\n",
        "    # Regex per trovare le tuple nel formato \"soggetto - verbo - oggetto\"\n",
        "    # Ignora il numero iniziale seguito da un punto o altri caratteri\n",
        "    pattern = r'\\d*\\.*\\s*(.*? - .*? - .*?)\\n'\n",
        "\n",
        "    # Trova tutte le tuple\n",
        "    matches = re.findall(pattern, before_note)\n",
        "\n",
        "    return matches"
      ],
      "metadata": {
        "id": "3EfB8cSiuzuN"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHfbvCSenpwi"
      },
      "source": [
        "####Debug singolo run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "IY3ks3DJejoa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14e0c1be-9ad8-44ba-a0bc-22960d35e19b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " <<SYS>>\n",
            "\n",
            "The following tuples represent the main actions and relationships in the given text:\n",
            "1. I - am not - a licensed healthcare professional\n",
            "2. I - cannot - diagnose or treat medical conditions\n",
            "3. Suggested strategies - are - for coping with depression\n",
            "4. One of the most effective strategies - is - to seek professional help\n",
            "5. Seeking Professional Help - can - help you identify the root causes of your depression and work with you to develop an individualized treatment plan\n",
            "6. Seeking Professional Help - may include - therapy, medication, or a combination of both\n",
            "7. Taking care of yourself - is - crucial in managing depression\n",
            "8.\n"
          ]
        }
      ],
      "source": [
        "user_input = f'''Read the following text and identify all the tuples in the subject-verb-object form. The tuples should reflect the main actions and relationships between the entities mentioned in the text. Follow these steps:\n",
        "\n",
        "              Identify the subject of the action.\n",
        "              Identify the verb that describes the action or relationship.\n",
        "              Identify the object or destination of the action.\n",
        "              Return the tuples in this format: Subject - Verb - Object.\n",
        "\n",
        "\n",
        "              Example:\n",
        "              Text: 'John gave a book to Mary.'\n",
        "              Tuple: 'John - gave - a book to Mary'\n",
        "              ##END LIST##\n",
        "\n",
        "              Apply this method to the following text:, {colonna_testo[9]} '''\n",
        "\n",
        "\n",
        "# Invia il messaggio e stampa la risposta\n",
        "response = model_run(user_input)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuples = extract_tuples(response)\n",
        "print(tuples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9OCMcus9PTh",
        "outputId": "c54ef394-96ce-4697-a1db-a069a975be44"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1. I - am not - a licensed healthcare professional', 'I - cannot - diagnose or treat medical conditions', 'Suggested strategies - are - for coping with depression', 'One of the most effective strategies - is - to seek professional help', 'Seeking Professional Help - can - help you identify the root causes of your depression and work with you to develop an individualized treatment plan', 'Seeking Professional Help - may include - therapy, medication, or a combination of both', 'Taking care of yourself - is - crucial in managing depression']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Run Iterato"
      ],
      "metadata": {
        "id": "GskMXLc5BXtf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "HKtGNF3nRG4B",
        "outputId": "38cfd657-6608-4b54-e604-b5f062f63742"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-3ec7ea31f166>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Clear GPU cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m   user_input = f'''Read the following text and identify all the tuples in the subject-verb-object form. The tuples should reflect the main actions and relationships between the entities mentioned in the text. Follow these steps:\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "#print(content)\n",
        "\n",
        "# Calculate runtime\n",
        "#runtime = end_time - start_time\n",
        "#print(\"response run time is: \", runtime)\n",
        "\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "for i in range(10):\n",
        "\n",
        "  torch.cuda.empty_cache() # Clear GPU cache\n",
        "  user_input = f'''Read the following text and identify all the tuples in the subject-verb-object form. The tuples should reflect the main actions and relationships between the entities mentioned in the text. Follow these steps:\n",
        "\n",
        "              Identify the subject of the action.\n",
        "              Identify the verb that describes the action or relationship.\n",
        "              Identify the object or destination of the action.\n",
        "              Return the tuples in this format: Subject - Verb - Object.\n",
        "              Print \"END LIST\" at the end of the list.\n",
        "\n",
        "              Example:\n",
        "              Text: 'John gave a book to Mary.'\n",
        "              Tuple: 'John - gave - a book to Mary'\n",
        "              ##END LIST##\n",
        "\n",
        "              Apply this method to the following text:, {colonna_testo[i]} '''\n",
        "\n",
        "\n",
        "  # Invia il messaggio e stampa la risposta\n",
        "  response = model_run(user_input)\n",
        "  print(response)\n",
        "  print(\"-----------------------FINE--------------------------------------\")\n",
        "  # Estrazione delle tuple complete\n",
        "  tuples = extract_tuples(response)\n",
        "\n",
        "  if i == 0:\n",
        "    # Creare un DataFrame\n",
        "    df_tuple = pd.DataFrame(tuples, columns=['TUPLA'])\n",
        "     # Aggiungere la colonna 'ID' con il valore di colonna_ID[0]\n",
        "    df_tuple['ID ARTICOLO'] = colonna_id[i]\n",
        "    df_tuple['CAMPAGNA'] = colonna_campagna[i]\n",
        "  else:\n",
        "    # Create temporary DataFrames for new data\n",
        "    df_temp = pd.DataFrame({'TUPLA': tuples, 'ID ARTICOLO': colonna_id[i], 'CAMPAGNA': colonna_campagna[i]})\n",
        "    # Concatenate the temporary DataFrame with the main DataFrame\n",
        "    df_tuple = pd.concat([df_tuple, df_temp], ignore_index=True)\n",
        "\n",
        "# Stop the timer\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate runtime\n",
        "runtime = end_time - start_time\n",
        "print(\"response run time is: \", runtime)\n",
        "print(df_tuple.head(50))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "8wS4JrU-RrtH",
        "outputId": "ad5737c0-4f35-4b5c-c7f5-c12820cd0eb5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_tuple' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-4f1c14beac8c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Salvare il DataFrame in un file Excel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_tuple\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TuplesExtracted_Q8_06TEMP.xlsx'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruntime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File Excel creato con successo!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_tuple' is not defined"
          ]
        }
      ],
      "source": [
        "# Salvare il DataFrame in un file Excel\n",
        "df_tuple.to_excel('TuplesExtracted_Q8_06TEMP.xlsx', index=False)\n",
        "print(runtime)\n",
        "print(\"File Excel creato con successo!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RT_Q5KM_0TEMP=371.50677728652954\n",
        "RT_Q5KM_03TEMP=385.1130666732788\n",
        "RT_Q5KM_06TEMP=394.9632875919342\n",
        "RT_Q8_0TEMP=340.24108028411865\n",
        "RT_Q8_03TEMP= 390.331839799881\n",
        "RT_Q8_06TEMP= 388.83690667152405"
      ],
      "metadata": {
        "id": "TZYg7oi2vPuU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}